{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7214eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Cleaning data...\n",
      "Normalizing data...\n",
      "Uploading data to MySQL...\n",
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re  # For name cleaning\n",
    "\n",
    "# Step 1: Load Raw Data\n",
    "def load_data():\n",
    "    print(\"Loading raw data...\")\n",
    "    encounters = pd.read_csv(\"encounters.csv\")\n",
    "    patients = pd.read_csv(\"patients.csv\", dtype={\"ZIP\": \"string\"})\n",
    "    organizations = pd.read_csv(\"organizations.csv\", dtype={\"ZIP\": \"string\"})\n",
    "    procedures = pd.read_csv(\"procedures.csv\")\n",
    "    payers = pd.read_csv(\"payers.csv\", dtype={\"ZIP\": \"string\"})\n",
    "    \n",
    "    # Store DataFrames in a dictionary for easy access\n",
    "    dataframes = {\n",
    "        \"encounters\": encounters,\n",
    "        \"patients\": patients,\n",
    "        \"organizations\": organizations,\n",
    "        \"procedures\": procedures,\n",
    "        \"payers\": payers\n",
    "    }\n",
    "    return dataframes\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "def clean_data(dataframes):\n",
    "    print(\"Cleaning data...\")\n",
    "        \n",
    "    # Parse and format date columns\n",
    "    datetime_columns = {\n",
    "        \"encounters\": [\"START\", \"STOP\"],\n",
    "        \"procedures\": [\"START\", \"STOP\"]\n",
    "    }\n",
    "    date_columns = [\"BIRTHDATE\", \"DEATHDATE\"]\n",
    "\n",
    "    # Handle datetime_columns using comprehension and update\n",
    "    for table_name, columns in datetime_columns.items():\n",
    "        df = dataframes[table_name]\n",
    "        df.update({\n",
    "            col: pd.to_datetime(df[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            for col in columns if col in df.columns\n",
    "        })\n",
    "\n",
    "    # Handle date_columns using comprehension and update\n",
    "    patients = dataframes[\"patients\"]\n",
    "    patients.update({\n",
    "        col: pd.to_datetime(patients[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "        for col in date_columns if col in patients.columns\n",
    "    })\n",
    "\n",
    "    # Clean names (FIRST, LAST, MAIDEN) in patients.csv using comprehension and update\n",
    "    patients.update({\n",
    "        col: patients[col].str.replace(r\"[^a-zA-Z]+\", \"\", regex=True)\n",
    "        for col in [\"FIRST\", \"LAST\", \"MAIDEN\"] if col in patients.columns\n",
    "    })\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Step 3: Normalize Data\n",
    "def normalize_data(dataframes):\n",
    "    print(\"Normalizing data...\")\n",
    "\n",
    "    encounters = dataframes[\"encounters\"]\n",
    "    patients = dataframes[\"patients\"]\n",
    "    organizations = dataframes[\"organizations\"]\n",
    "    procedures = dataframes[\"procedures\"]\n",
    "    payers = dataframes[\"payers\"]\n",
    "\n",
    "    # Normalize Patients\n",
    "    unique_patients = patients.drop_duplicates(subset=\"Id\")\n",
    "    patient_map = unique_patients.reset_index(drop=True).reset_index().assign(patient_id=lambda x: x[\"index\"] + 1)\n",
    "    encounters[\"patient_id\"] = encounters[\"PATIENT\"].map(patient_map.set_index(\"Id\")[\"patient_id\"])\n",
    "    procedures[\"patient_id\"] = procedures[\"PATIENT\"].map(patient_map.set_index(\"Id\")[\"patient_id\"])\n",
    "\n",
    "    # Normalize Organizations\n",
    "    unique_orgs = organizations.drop_duplicates(subset=\"Id\")\n",
    "    org_map = unique_orgs.reset_index(drop=True).reset_index().assign(organization_id=lambda x: x[\"index\"] + 1)\n",
    "    encounters[\"organization_id\"] = encounters[\"ORGANIZATION\"].map(org_map.set_index(\"Id\")[\"organization_id\"])\n",
    "\n",
    "    # Normalize Payers\n",
    "    unique_payers = payers.drop_duplicates(subset=\"Id\")\n",
    "    payer_map = unique_payers.reset_index(drop=True).reset_index().assign(payer_id=lambda x: x[\"index\"] + 1)\n",
    "    encounters[\"payer_id\"] = encounters[\"PAYER\"].map(payer_map.set_index(\"Id\")[\"payer_id\"])\n",
    "\n",
    "    # Normalize Encounters\n",
    "    unique_encounters = encounters.drop_duplicates(subset=\"Id\")\n",
    "    encounters = unique_encounters.reset_index(drop=True).reset_index().assign(encounter_id=lambda x: x[\"index\"] + 1)\n",
    "    procedures[\"encounter_id\"] = procedures[\"ENCOUNTER\"].map(encounters.set_index(\"Id\")[\"encounter_id\"])\n",
    "\n",
    "    # Drop old primary and foreign keys\n",
    "    patient_map = patient_map.drop(columns=[\"Id\", \"index\"])\n",
    "    org_map = org_map.drop(columns=[\"Id\", \"index\"])\n",
    "    payer_map = payer_map.drop(columns=[\"Id\", \"index\"])\n",
    "    encounters = encounters.drop(columns=[\"PATIENT\", \"ORGANIZATION\", \"PAYER\", \"Id\", \"index\"]) \n",
    "    procedures = procedures.drop(columns=[\"PATIENT\", \"ENCOUNTER\"])\n",
    "\n",
    "    # Reorder columns\n",
    "    encounters = encounters[[\n",
    "        \"encounter_id\", \"START\", \"STOP\", \"patient_id\", \"organization_id\", \"payer_id\",\n",
    "        \"ENCOUNTERCLASS\", \"CODE\", \"DESCRIPTION\", \"BASE_ENCOUNTER_COST\",\n",
    "        \"TOTAL_CLAIM_COST\", \"PAYER_COVERAGE\", \"REASONCODE\", \"REASONDESCRIPTION\"\n",
    "    ]]\n",
    "    organizations = org_map[[\"organization_id\"] + [col for col in org_map.columns if col != \"organization_id\"]]\n",
    "    patients = patient_map[[\"patient_id\"] + [col for col in patient_map.columns if col != \"patient_id\"]]\n",
    "    payers = payer_map[[\"payer_id\"] + [col for col in payer_map.columns if col != \"payer_id\"]]\n",
    "    procedures = procedures[[\"START\", \"STOP\", \"patient_id\", \"encounter_id\"] + [col for col in procedures.columns if col not in [\"START\", \"STOP\", \"patient_id\", \"encounter_id\"]]]\n",
    "\n",
    "    # Convert column names to lowercase\n",
    "    encounters.columns = encounters.columns.str.lower()\n",
    "    organizations.columns = organizations.columns.str.lower()\n",
    "    patients.columns = patients.columns.str.lower()\n",
    "    payers.columns = payers.columns.str.lower()\n",
    "    procedures.columns = procedures.columns.str.lower()\n",
    "\n",
    "    return {\n",
    "        \"patients\": patients,\n",
    "        \"organizations\": organizations,\n",
    "        \"payers\": payers,\n",
    "        \"procedures\": procedures,\n",
    "        \"encounters\": encounters\n",
    "    }\n",
    "\n",
    "# Step 4: Upload to MySQL\n",
    "def upload_to_mysql(normalized_data):\n",
    "    print(\"Uploading data to MySQL...\")\n",
    "\n",
    "    # Replace with local MySQL credentials\n",
    "    engine = create_engine(\"mysql+pymysql://root:ENTERPASSWORD@localhost/hospital_patient_records\")\n",
    "\n",
    "    # Upload normalized tables\n",
    "    normalized_data[\"patients\"].to_sql(\"patients\", con=engine, if_exists=\"replace\", index=False)\n",
    "    normalized_data[\"organizations\"].to_sql(\"organizations\", con=engine, if_exists=\"replace\", index=False)\n",
    "    normalized_data[\"payers\"].to_sql(\"payers\", con=engine, if_exists=\"replace\", index=False)\n",
    "    normalized_data[\"procedures\"].to_sql(\"procedures\", con=engine, if_exists=\"replace\", index=False)\n",
    "    normalized_data[\"encounters\"].to_sql(\"encounters\", con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    print(\"Upload complete!\")\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Load raw data\n",
    "    dataframes = load_data()\n",
    "\n",
    "    # Clean data\n",
    "    dataframes = clean_data(dataframes)\n",
    "\n",
    "    # Normalize data\n",
    "    normalized_data = normalize_data(dataframes)\n",
    "\n",
    "    # Upload data to MySQL\n",
    "    upload_to_mysql(normalized_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
